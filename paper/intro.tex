\section{\label{sec:intro}Introduction}

Very often expressiveness of a programming language introduces a number of
ambiguities in its syntax.  The language specification clearly states how to
resolve the conflict, however it may not be possible to formulate the
resolution in terms of context free grammars.  In order to illustrate that we
present the following examples.

\begin{enumerate}
    \item The classical example from C language is a type-cast 
          syntax.  As a user can define an arbitrary type using
          \verb|typedef| construct, evaluation of the expression
          \verb|(x) + 5| is impossible, unless we know if
          \verb|x| is a type or not.
    \item Assume that we extend C syntax to allow array 
          concatenation using infix binary \verb|++| operator and
          constant-arrays to be written as \verb|[1, 2, 3]|.
          We immediately run into the problem to disambiguate the 
          following expression: \verb|a ++ [1]|, as it could mean
          an application of postfix \verb|++| indexed by \verb|1|
          or it could be an array concatenation of \verb|a| and 
          \verb|[1]|.
%    \item Assuming the language allows any unary function or operator
%          to be applied as infix and postfix, we cannot 
%          disambiguate the following expression:
%\begin{verbatim}
%log (x) - log (y)
%\end{verbatim}
%          Potential interpretations are: 
%          \verb|log (uminus (log (x))) (y)|, which is obviously an 
%          error, or \verb|minus (log (x), log (y))|.
\end{enumerate}

\noindent
Sometimes the context may influence not only the parsing decisions but 
also lexing decisions.  Consider the following examples:
\begin{enumerate}
    \item C++ allows nested templates, which means that one could
          write an expression \verb|template <typename foo, list <int>>|, 
          assuming that the \verb|>>| closes the two groups.  In
          order to do that, the lexer must be aware of this context,
          as in a standard context character sequence \verb|>>| means
          shift right operation.
    \item Assuming that a programmer is allowed to define her own 
          operators, the lexer rules must be changed, in case 
          the name of a new operator extends the existing one.  For
          example, assume one defines an operation \verb|+-|.
          It means that from now on an expression \verb|+-5| should
          be lexed as (\verb|+-|, \verb|5|), rather than (%
          \verb|+|, \verb|-|, \verb|5|).
\end{enumerate}

\noindent
In order to resolve the above ambiguities using table-based parser generator,
we have to make sure that one can annotate the grammar with correct choices for
each shift/reduce or reduce/reduce conflict. This puts a number of requirements
on the syntax of a parser generator and on the finite-state machine execution
engine.  Firstly, one has to introduce contexts without interfering with the
above conflict-resolution.  Secondly, one has to have an interface to the lexer,
in case lexing becomes context-dependent, and all the mechanisms should be aware 
of error-recovery facilities.

Having said that, we can see that using parser generators could be of
the same challenge as writing a parser manually, where all the
ambiguities could be carefully resolved according to the language
specification.  As it turns out, most of the real-world language
front-ends use hand-written recursive descent parsers that specially
treat cases that cause ambiguity.  For example the following languages do:
C/C++/ObjectiveC in GNU GCC~\cite{gcc}, clang in LLVM~\cite{clang}, 
JavaScript in Google V8~\cite{v8}.

The main goal of any preprocessor is to perform a substitution of
one element sequence with another.  The unit of the sequence may 
be different depending on the agreement, however the common
case is to say that the unit is a sequence of characters of the
same class.  The number of classes is normally fixed, however 
character belonging to the class may be static as in C preprocessor,
where, for example, notion of space cannot be changed, or dynamic as
in \TeX, where one could specify that a certain character is a 
delimiter.  Then the substitution itself is a replacement of 
units in a sequence, which are treated as arguments, with the 
assigned arguments.  The key problem here, as we are concerned
in this paper, is a lack of separation between the rewriting itself
and the transformation of a token-sequence.  Consider an example of
a C macro:
\begin{verbatim}
#define foo(x, y) x y
\end{verbatim}
First of all, it is really hard to say anything about the result
of this macro, as \verb|foo (5,6)| expands to \verb|5 6|, but
both \verb|foo (,5)| and \verb|foo (5,)| expands to \verb|5|.
Secondly, as comma is a part of syntax definition of the macro
then one cannot just pass a token sequence \verb|5, 6| as a
first argument of \verb|foo|.  In order to resolve this one
may escape the comma by wrapping it in parentheses and calling
\verb|foo ((5,6), 7)| which will expand to \verb|(5, 6) 7|.
The only way to flattern the list is to perform an application
of another macro.  For example:
\begin{verbatim}
#define first(x, y) x
#define bar(x, y) first x y
\end{verbatim}
So we have higher-order macro here, but it would work only if
arguments have a right type and the application of 
\verb|bar ((5,6), x)| would expand to \verb|5, x|, however
application of \verb|bar (5, 6)| would not provide an error
but would expand to \verb|first 5 6|.  And as a last example
we can make original macro \verb|foo| return 3 arguments
by expanding \verb|foo (5, foo (6, 7))| which will expand to
\verb|5 6 7|.  We may clearly see that making some static 
conclusions by checking a system of macros is impossible, as
it may all depend on the application; and making any dynamic
decisions with respect to the correctness of substitution is
also not possible, as there is no way to declare the criteria
of correctness.

Despite all the correctness complications macro systems are not powerful
enough to introduce new language constructs.  For example, it would be natural
to represent a number's absolute value as \verb/|a|/, or to allow a number of
user-defined literals to introduce units in a programming language like 
\verb/5kg/ or \verb|8 mm|.  Even if a macro-system can do it, resolving nested
expressions treating one and the same symbol differently still might be
confusing.  For example, would it be possible for some macro system to
transform an expression \verb/| a|b |/ into \verb/abs (a|b)/?

The proper way of doing macro-substitutions is to allow an
extension to the grammar.  However, providing a handle for
arbitrary changes of the grammar may lead to uncontrolled 
changes in the semantics of the language, which again would
make the proof of program corectness hard to create.

As a solution to the given problem we introduce a preprocessor
which works on the sequence of pseudo-tokens which is a combination
of tokens recognized by parser and productions of the grammar
used by parser.  To make it even more powerful we allow a 
regular expression on pseudo-tokens, still being able to 
guarantee the correctness of the transformation.

The rest of the paper is organized as follows: we are going to 
introduce a basic model of the parser which we use to build a
preprocessor on in section 2.  Then we describe the syntax of
the preprocessing rules and the way we intend to prove correctness
in section 3.  Section 4 describes the way we deal with a number
of preprocessor definitions and finally we evaluate the work
and conclude in sections 5 and 6.


