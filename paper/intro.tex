\section{\label{sec:intro}Introduction}

Very often languages like C/C++ introduce new constructs in their
syntax.  Some of the constructs require serious modifications of the
compiler to be supported properly, but some of them may be purely syntactical,
like, for example, user-defined literals in C++11.  In this paper we are
concerned with a system that allows a class of modifications at the 
level of the programming language itself, without the necessity to
modify the compiler.

The necessity to change the compiler in order to support syntactical 
changes comes from the fact that most of the programming languages 
have restricted capabilities for changing syntax.  Some of them do not
provide any at all.  We believe that the key
aspect to the proper self-modifying language is an ability to 
change a grammar on the fly.  If so, one can say that the
reasonable approach might be to create a cross compiler which
would transform a desired syntax into the syntax recognized by
some standard compiler.  The problem with this approach is, that most of
the base-line languages come with a syntax that is very difficult
to parse using an automatic tool.  As an example consider the 
following cases.


%Very often expressiveness of a programming language introduces a number of
%ambiguities in its syntax.  The language specification clearly states how to
%resolve the conflict, however it may not be possible to formulate the
%resolution in terms of context free grammars.  In order to illustrate that we
%present the following examples.

\begin{enumerate}
    \item %The classical example from C language is a type-cast 
          %syntax.  
          In C, a user can define an arbitrary type using the
          \verb|typedef| construct, which makes it impossible 
          to disambiguate the expression \verb|(x) + 5|, unless
          we know whether \verb|x| is a type or not. It could
          be treated as a type-casting of an application of unary
          plus to \verb|5| to the type called \verb|x|,
          or a sum of variable called \verb|x| with \verb|5|.
    \item If we were allowed to extend C syntax with an infix 
          binary array concatenation operation denoted \verb|++|, and
          constant-arrays were allowed to be written as 
          \verb|[1, 2, 3]|,
          we  would immediately run into the problem of disambiguating
          the following expression: \verb|a ++ [1]|.  It could mean
          an application of postfix \verb|++| indexed by \verb|1|
          or it could be an array concatenation of \verb|a| and 
          \verb|[1]|.
%    \item Assuming the language allows any unary function or operator
%          to be applied as infix and postfix, we cannot 
%          disambiguate the following expression:
%\begin{verbatim}
%log (x) - log (y)
%\end{verbatim}
%          Potential interpretations are: 
%          \verb|log (uminus (log (x))) (y)|, which is obviously an 
%          error, or \verb|minus (log (x), log (y))|.
\end{enumerate}

\noindent
Sometimes the context may influence not only the parsing decisions but 
also lexing decisions.  Consider the following examples:
\begin{enumerate}
    \item C++ allows nested templates, which means that one could
          write an expression \verb|template <typename foo, list <int>>|, 
          assuming that the \verb|>>| closes the two groups.  In
          order to do that, the lexer must be aware of this context,
          as in a standard context character sequence \verb|>>| means
          shift right operation.
    \item Assuming that a programmer is allowed to define her own 
          operators, the lexer rules must be changed, in case 
          the name of a new operator extends the existing one.  For
          example, assume one defines an operation \verb|+-|.
          It means that from now on an expression \verb|+-5| should
          be lexed as (\verb|+-|, \verb|5|), rather than (%
          \verb|+|, \verb|-|, \verb|5|).
\end{enumerate}

\noindent
In order to resolve the above ambiguities using a grammar-based parser generator,
we have to make sure that one can annotate the grammar with correct choices for
each shift/reduce or reduce/reduce conflict.  This puts a number of requirements
on the syntax of a parser generator and on the finite-state machine execution
engine.  Firstly, one has to introduce contexts without interfering with the
above conflict-resolution.  Secondly, one has to have an interface to the lexer,
in case lexing becomes context-dependent, and all the mechanisms should be aware 
of error-recovery facilities.

Having said that, we can see that using automatic parser generators could be of
the same challenge as writing a parser manually, where all the
ambiguities could be carefully resolved according to the language
specification.  As it turns out, most of the real-world language
front-ends use hand-written recursive descent parsers that specially
treat cases that cause ambiguity.  For example the following languages do:
C/C++/ObjectiveC in GNU GCC~\cite{gcc}, clang in LLVM~\cite{clang}, 
JavaScript in Google V8~\cite{v8}.

That indicates that the first step of cross-compilation
would be actually a re-creation of the parser of the base-line language.
As it requires a major undertake to create such a grammar, and
afterwards one has to update the grammar with the changes made 
in the original parser, we do not seriously consider this approach.
Instead we are going to design a system, which uses an existing
parser as a base-line and introduces a set of handles to modify
the grammar on the fly.  However, arbitrary changes of the grammar may lead
to an uncontrolled language evolution which would be hard to verify,
so in our approach we allow a restrictive set of changes to the grammar
and use a specially designed type-system to control the correctness
of transformations.

Very similar tasks, maybe except the correctness verification, are
performed by generic macro-processors.  Now, is it possible to
build our system on one of the existing preprocessing engines,
preferably providing some correctness guarantees?

The main goal of any preprocessor is to perform a substitution of
one element sequence with another.  The unit of the sequence may 
be different depending on the agreement, however the common
case is to say that the unit is a sequence of characters of the
same class.  The number of classes is normally fixed, however 
character belonging to the class may be static as in C preprocessor,
where, for example, notion of space cannot be changed, or dynamic as
in \TeX, where one could specify that a certain character is a 
delimiter.  Then the substitution itself is a replacement of 
units in a sequence, which are treated as arguments, with the 
assigned arguments.  The key problem here, as we are concerned
in this paper, is a lack of separation between the rewriting itself
and the transformation of a token-sequence.  Consider an example of
a C macro:
\begin{verbatim}
#define foo(x, y) x y
\end{verbatim}
First of all, it is really hard to say anything about the result
of this macro, as \verb|foo (5,6)| expands to \verb|5 6|, but
both \verb|foo (,5)| and \verb|foo (5,)| expand to \verb|5|.
Secondly, as comma is a part of syntax definition of the macro
then one cannot just pass a token sequence \verb|5, 6| as a
first argument of \verb|foo|.  In order to resolve this one
may escape the comma by wrapping it in parentheses and calling
\verb|foo ((5,6), 7)| which will expand to \verb|(5, 6) 7|.
The only way to flattern the list is to perform an application
of another macro.  For example:
\begin{verbatim}
#define first(x, y) x
#define bar(x, y) first x y
\end{verbatim}
So we have a higher-order macro here, but it would work only if
arguments have a right type and the application of 
\verb|bar ((5,6), x)| would expand to \verb|5, x|, however
application of \verb|bar (5, 6)| would not provide an error
but would expand to \verb|first 5 6|.  And as a last example
we can make original macro \verb|foo| return 3 arguments
by expanding \verb|foo (5, foo (6, 7))| which will expand to
\verb|5 6 7|.  We may clearly see that making some static 
conclusions by checking a system of macros is impossible, as
it may all depend on the application; and making any dynamic
decisions with respect to the correctness of substitution is
also not possible, as there is no way to declare the criteria
of correctness.

Despite all the correctness complications macro systems are not powerful
enough to introduce new language constructs.  For example, it would be natural
to represent a number's absolute value as \verb/|a|/, or to allow a number of
user-defined literals to introduce units in a programming language like 
\verb/5kg/ or \verb|8 mm|.  Even if a macro-system can do it, resolving nested
expressions treating one and the same symbol differently still might be
confusing.  For example, would it be possible for some macro system to
transform an expression \verb/| a|b |/ into \verb/abs (a|b)/?


%The proper way of doing macro-substitutions is to allow an
%extension to the grammar.  However, providing a handle for
%arbitrary changes of the grammar may lead to uncontrolled 
%changes in the semantics of the language, which again would
%make the proof of program corectness hard to create.

So we just saw that using a preprocessor as it is for the desired task 
does not exactly meet our intentions.
As a solution to the given problem we introduce a transformation system
which works on the sequence of pseudo-tokens, which is a combination
of tokens recognized by parser and productions of the grammar
used by parser.  To make it even more powerful we allow a 
regular expression on pseudo-tokens, still being able to 
guarantee the correctness of the transformation.

The rest of the paper is organized as follows: we are going to 
introduce a basic model of the parser which we use to build a
transformation system on in section 2.  Then we describe the syntax of
the transformation rules and the way we intend to prove correctness
in section 3.  Section 4 describes the way we deal with multiple 
transformation rules and finally we evaluate and conclude 
the work in sections 5 and 6.
