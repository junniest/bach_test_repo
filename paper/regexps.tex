\section{\label{sec:regexps}Regular expressions}

We have devised a method of implementing match syntax that will be time
effective during pre-processing. We have also created a prototype that
ilustrates some of the features described in this article. This section will,
in short, describe the approach we selected, for the full description, however,
turn to the full paper.

As it was said, match syntax is a simple regular expression. This expression is
parsed and transformed into a non-deterministic finite automaton (NFA). Next
step is to create a deterministic finite automaton (DFA) out of the created NFA
to minimize the effort needed to execute the match. This task is performed
using the subset construction algorithm.

In order to minimise execution time even further, we minimise the created DFA.
The minimisation algorithm is described in detail in the "Dragon Book". The
algorithm creates sets of states that cannot be distinguished by any input
token sequence. Once the algorithm fails to break the sets into smaller ones
it stops. These sets of states then become new states of the minimal automaton.

It is essential to note that the existence of such minimal automaton is
provable, despite the complexity of the regular expression it describes. This
statement implies that we have a possibility to prove equality of two automata.
As the naming of the states is unimportant, we will say that two automata are
the same up to state names if one can be transformed to another be simply
renaming the states. Therefore two regular expressions match the same input if
and only if their automata are the same up to state names.

The construction of the DFA does not support back-referencing, so the bracket
groups are intended only to change the priority of the operations in the
regular expressions. We abandon the back-referencing feature in favor of
maintaining a fully determinate automaton for each regular expression.

\subsection{DFA grouping}
As we operate with source code, we want to check all of the matches that we
have in one pass through the code. We consider two possibilities to merge the
created automata into a match system in order to improve effieciency. For both
of them we evaluate DFA adding, matching and context inheritance algorithmic
difficulty in the main article.

Context inheritance difficulty is important, as we can have several included
contexts, where the matches introduced inside the included one have a higher
priority. Imagining that we have a system of \verb/n/ automata we have two
options of doing so. The first one is that on entering a context we add the
\verb/m/ matches we come across to the main match set and remove the \verb/m/
matches upon exiting the context. The second one is that on entering a context
we create a copy of the parent context, to which we add the \verb/m/ newly
found matches and upon exiting the context throw out the entire new system of
matches.

First of the options we consider is joining the automata in a list. Let us
assume that there are \verb/n/ matches to be checked. Then the automata list
represents an NFA with \verb/n/ epsilon branches from the start state, each of
which leads to one of the DFA we already created. This case is fairly simple
and will not be described here extensively.

The second option is more complex. In this case we combine the automata created
for each of the matches together into a single DFA. This is done in order to
reduce matching time, the automata megring algorithm is described in the full
article.  The second option is more interesting, as it allows matching of
\verb/n/ independent patterns in $O(l)$ steps, where \verb/l/ is the amount of
tokens to be matched.

We do not explicitly select a method of operation with joining DFA in the
article, we only give the complexities of the proposed variants. This is
because the optimal solution selection should be based on the practical uses of
the system. It is impossible to state, without any actual examples, what will
be more time-effective during execution. Even though the second option is
time-consuming when adding and removing matches, the dramatic improvement in
execution time might come from the fact that match count is relatively small,
but the automaton execution will, at worst cases, be performed for every token
in the source text.

