\section{\label{sec:dynext}Transformation system}

In this section we are going to describe a syntax of the rules of 
the transformation system and demonstrate the way to prove a correctness
of the transformation.

\subsection{Match Syntax}

The transformation system consists of the \textit{match} rules and
token-transformation functions.  First of all we would consider the
match rule, which modifies a behaviour of a given production.  The
syntax of the rule can be learned from the following example.
\begin{verbatim}
match [\prod1] v = regexp   ->   [\prod2] f (v)
\end{verbatim}
This reads as follows: if at the beginning of production \verb|prod1|
a stream of pseudo-tokens pointed at by the parser matches a regular expression
\verb|regexp|, which can be aliased with variable named \verb|v| in the
right hand side of the match, then the matched tokens will be replaced 
with a reduction of \verb|prod2| production applied on a list of 
pseudo-tokens that is being returned by \verb|f (v)|.  Function \verb|f|
is a function which is defined in functional language $T$ and which
is used to perform a transformation on the list of tokens matched by
the left hand side of the match.  

The \verb|regexp| regular expression is a box standard regular
expression~\cite{regexp} which is defined by the grammar at 
Fig.~\ref{fig:reggram}.
\begin{figure}[h!]
\begin{verbatim}
regexp          ::= concat-regexp '|' regexp
concat-regexp   ::= asterisk-regexp  concat-regexp
asterisk-regexp ::= unary-regexp '*' | unary-regexp
unary-regexp    ::= pseudo-token | '(' regexp ')'
\end{verbatim}
\caption{\label{fig:reggram}Grammar of the regular expressions on
pseudo-tokens}
\end{figure}
In this paper we are using a minimalistic syntax for regular expression
to demonstrate some basic properties.  Later on, this syntax may be 
easily extended.

Further down in this paper we are going to use an escaped syntax 
for pseudo-tokens which represent grammar production names, like
\verb|\expr|, \verb|\function|, etc.  The operators of regular
expression, namely (\verb/|/, \verb|*|, \verb|(|, \verb|)|) will
be escaped as well like: (\verb/\|/, \verb|\*|, \verb|\(|, \verb|\)|).
For simplicity reasons we assume that we do not introduce
any conflicts by defining escape symbols.  In case a conflict 
appears, we may change the escape symbol, or even the whole escaping
mechanism, but it does not influence the matter.

Now we can demonstrate a simple substitution example on the language
defined in Fig.~\ref{fig:grammar}.  Assume that function \verb|replace|
is defined in $T$ with three arguments and it replaces in any list 
of pseudo-tokens occurrence each of the second argument with the third, 
and what we need is to call a function called \verb|bar| with an 
argument being a summed-up arguments of function called \verb|foo|.
In that case the following match would perform such a substitution.
\begin{verbatim}
match [\fun_call] v = foo ( \expr \( , \expr \) \* )
   -> [\fun_call] cons bar (replace (tail v) \, \+)
\end{verbatim}

\subsection{Definition of matches}
Match rules can be defined in the arbitrary places of a program
and the rule activates immediately after the definition was parsed.
We do however differentiate between the global matches and context
matches.  In our case the context is created by a \verb|stmt_block|
production.  In that case, all the matches declared within the 
\verb|stmt_block| production are valid only within this particular
production.  When the production is finished, the matches declared
within the production would be removed.  Declaration of the context
is up to the parser, it may define it in any which way by calling 
two interface functions.  The context definition can be omitted in
which case all the matches would be global.

Both global and context matches depend on the order of definition
and in both cases the first match has a stronger priority than the
subsequent in case the token stream can be matched with more than
one regular expression.  The priority helps to cover the following
case:
\begin{verbatim}
match [\expr] v = f ( \num )            ...
mathc [\expr] v = f ( \primary_expr )   ...
match [\expr] v = f ( \expr )           ...
\end{verbatim}
Here we can see that the last regular expression includes the 
previous as \verb|\primary_expr| is also an \verb|\expr| and
so on.  But introducing priorities, one may still have a different
behaviour in each case.

Nested context matches overload the outer matches, in a same
way as local variables have a first match, in case a variable
of the same name exists in the outer context.

%The match definition is rejected with an error in case the system
%can prove an infinite loop there.  It may happen in case of left
%recursion in the left-hand side of the match and in case when 
%the left-hand side production and left-hand side regular expression
%of the match 

\subsection{$T$ language and correctness}
In order to perform a transformation of the matched list we define a
minimalistic functional language called $T$ in order to demonstrate
the approach.  The core definition of $T$ is given by Fig.~\ref{fig:t}.

\begin{figure}
\begin{verbatim}
program         ::= ( function ) *
function        ::= id '::' fun_type id id * '=' expr
fun_type        ::= (type | '(' fun_type ')') '->' fun_type
expr            ::= id | expr expr | let_expr | if_expr | builtin
let_rec         ::= 'let' id '=' expr (',' id '=' expr) * 'in' expr
if_expr         ::= 'if' cond_expr 'then' expr 'else' expr
cond_expr       ::= 'type' expr '==' type | expr
builtin         ::= 'cons' expr  (expr | 'nil') 
                    | 'head' expr | 'tail' expr | 'value' expr
                    | pseudo_token '[' expr ']' | number
                    | + | - | ...
type            ::= pseudotoken_regexp | int | regexp_t
\end{verbatim}
\caption{\label{fig:t}Grammar to define language $T$}
\end{figure}

The main use-case of the language is to traverse over the matched
list of pseudo-tokens applying recursion, head, tail and cons
constructs.  In order to stop the recursion we also introduce
arithmetic operations on integers.  In order to perform partial
evaluation, we need to have an interface to the value of the
pseudo-token.  For that reason we introduce function \verb|value|
which is applicable to the pseudo-tokens which have a constant
integer value (in our example it is a \verb|\number| pseudo-token).
In order to construct an object from integer, we are using
\verb|\number[42]| syntax.  The \verb|value| function operates
on integers only for the simplicity of the model only, the basic
types can be extended in future.

\subsubsection{Type system}
In order to prove the correctness of the match, we are going to use
a specially designed type-system which is based on the regular 
expressions being treated as types.  We are going to use a type
inference to check if the function application in the right hand
side of the match is allowed within a given production.  In the
current paper we are not going to give a definition of all the
type deduction rules, however, we are going to do that in the 
full paper.  Further in this section we are going to share the
basic ideas and principles.

The main idea of the type system for $T$ is to treat a regular
expression as a type.  We start with a fact that if the 
right-hand side of the system was called and the match succeeded
then the result of the match is a flat list of pseudo-tokens.
However from the regular expression we have additional information
about the structure of this list.  In order to perform a type 
inference, we observe that regular languages, hence regular 
expression bring a number of set operations, which is the key
driving force of the inference.  First of all, it is easy to
define a subset relationship on two regular expressions
$r_1 \sqsubseteq r_2$.  As we know, we can always build a DFA for
a regular expression and minimize it~\cite{dragon-book}, which gives us a minimal
possible automaton for the language recognized by a given regular
expression.  It means that $r_1 \sqsubseteq r_2 \Rightarrow 
min (det (r_1)) \sqsubseteq min (det (r_2))$.  For two minimized 
automata $A_1$ and $A_2$, $A_1 \sqsubseteq A_2$ means that there
is a mapping $\Psi$ of $A_1$ states to $A_2$ states such that:
\[
    Start (A_1) \to Start (A_2) \in \Psi
\]
\[
    \forall s \in States (A_1) \forall e \in Edges (s),
    \Psi (Transition (s, e)) = Transition (\Psi (s), e)
\]

$States (x)$ denotes a set of all the states of automaton $x$,
$Edges (s)$ is a set of pseudo-tokens which mark the outcoming
edges of state $s$.  Finally, $Transition (s, t)$ denotes a
state which is reachable from $s$, using edge marked with $t$.

The subset relationship is going to be used to create a sub-typing
hierarchy, and we also have a notion of a super-type of
the hierarchy, which is represented by a regular expression \verb|.*|,
we are going to denote this type $\top$.  It is obvious to see that
$\forall t_i \in R, t_i \sqsubseteq \top$.

It is possible to construct a type for head and tail application
by following the edges from the starting state of DFA in case of
head, and creating a set of sub-automata in case of tail.
Furthermore we can infer a type for recursive head/tail traversal
over the list in either direction.  In order to do that, one has
to construct a set of all the sub-automata of a given one in 
case of forward traversal and the set of all sub-automata of
the reversed automaton in case of backward.

In order to propagate type information in the branches obtained from the
application of \verb|type| construct, we have to know how to subtract
two regular expressions~\cite{reglang}, which is, again, simple.  
If $T$ and $S$  are
respective DFA for subtracted types, all we have to do is to construct
$C = T \times S$ and make the final states of $C$ be the pairs where $T$
states are final and $S$ states are not. 

The type inference procedure itself borrows the idea from inferring
the shape of the array in SaC type system~\cite{sac2c} i.e. we will
start with an abstract type \verb|regexp_t|
which is $\top$ and recursively precise the type until we get to
a fixed point.







